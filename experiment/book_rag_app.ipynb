{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7628b89f-7ee2-44a0-85bb-eb8a5f72f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frodo/anaconda3/envs/parikshit/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    PromptTemplate,\n",
    ")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "import torch\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee36305c-b425-4bd2-a7fb-eb48c3c02a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "tokenizer.eos_token_id,\n",
    "tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2f0f44-2e8e-4b78-9ec3-a7986fc1cf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128009, 128009]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopping_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1689db-a969-4d6f-b280-0ba95166c7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc4c0ab920d49aaa618c2d6a6806f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    model_kwargs={'trust_remote_code':True},\n",
    "    generate_kwargs={\"do_sample\": False},\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=stopping_ids,\n",
    ")\n",
    "\n",
    "embed_model= HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73683013-364a-44e7-961a-7f046c94b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = embed_model\n",
    "Settings.llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c254b94f-f257-4ab1-b116-93ee3b12ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext, VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./book/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088704de-5cea-4ebd-af3b-28dd381656e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce21e30c-879f-4359-bc57-71e684e0fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create senetence window node parser with default settings\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser,SimpleNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "# Create sentence window node parser with default settings\n",
    "sentence_node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "sentence_nodes = sentence_node_parser.get_nodes_from_documents(documents)\n",
    "sentence_index = VectorStoreIndex(sentence_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c899d66-ed64-4936-b768-5bc3a8c1ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = CitationQueryEngine.from_args(\n",
    "    sentence_index,\n",
    "    citation_chunk_size=512,\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e29b218-cedb-4c03-9e43-e2941c9986d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frodo/anaconda3/envs/parikshit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/frodo/anaconda3/envs/parikshit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is taming intuitive predictions ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1cf4270-89e9-4e62-a44c-4bea0792f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Taming intuitive predictions requires a significant effort of self-monitoring and self-control [2]. It involves accepting the overall forecast of economists and not making unwarranted causal inferences [1]. It also involves recognizing the limitations of intuitive predictions and the importance of discipline in constraining our beliefs by the logic of probability [2]. \n"
     ]
    }
   ],
   "source": [
    "print(response.response.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "477c9b88-f0b2-46df-ba02-28653379f879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['window', 'original_text', 'page_label', 'file_name', 'file_path', 'file_type', 'file_size', 'creation_date', 'last_modified_date'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0].metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a91aa04-504e-4055-8c2b-d62a475304a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metakeys = ['page_label', 'file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date']\n",
    "meta_text = \"\"\n",
    "for key in metakeys:\n",
    "    meta_text+=f\"{key}: {response.source_nodes[0].metadata[key]}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397cb7ff-99cd-44b8-a72f-64489c0e9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 180\n",
      "file_name: Daniel Kahneman-Thinking, Fast and Slow  .pdf\n",
      "file_type: application/pdf\n",
      "file_size: 3675247\n",
      "creation_date: 2024-07-22\n",
      "last_modified_date: 2024-07-22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(meta_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29894ba-0114-412f-9e49-05798101e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is taming intuitive predictions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f845a584-3ed4-4750-b015-6e8e86968543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://192.168.0.44:8895\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://192.168.0.44:8895/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frodo/anaconda3/envs/parikshit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/frodo/anaconda3/envs/parikshit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "# Global variable to store citation_text temporarily\n",
    "temporary_citation_text = \"\"\n",
    "\n",
    "# Function to handle query and return response and citations\n",
    "def search_query(query):\n",
    "    global temporary_citation_text\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    response_text = response.response.split(\"\\n\")[0]\n",
    "\n",
    "    pattern = r'\\[(\\d+)\\]'\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, response_text)\n",
    "\n",
    "    citation_idx = set()\n",
    "    for match in matches:\n",
    "        citation_idx.add(int(match)-1)\n",
    "        \n",
    "    citation_idx = list(citation_idx)\n",
    "    print(citation_idx)\n",
    "    \n",
    "    citations = []\n",
    "    metakeys = ['page_label', 'file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date']\n",
    "    for cit in citation_idx:\n",
    "        meta_text = \"\"\n",
    "        for key in metakeys:\n",
    "            meta_text+=f\"{key}: {response.source_nodes[cit].metadata[key]}\\n\"\n",
    "        citations.append(response.source_nodes[cit].get_text()+\"\\n\"+meta_text)\n",
    "        \n",
    "    citation_text = \"\\n===========================================\\n\".join([f\"{citation}\" for i, citation in enumerate(citations)])\n",
    "    \n",
    "    # Store the citation text temporarily\n",
    "    temporary_citation_text = citation_text\n",
    "    \n",
    "    # Return response text and make the button interactive\n",
    "    return response_text, gr.update(interactive=True)\n",
    "\n",
    "# Function to display citations when the button is clicked\n",
    "def show_citations():\n",
    "    global temporary_citation_text\n",
    "    return temporary_citation_text\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            query_input = gr.Textbox(label=\"Query\")\n",
    "            response_text = gr.Textbox(label=\"Response\", interactive=False)\n",
    "        with gr.Column():\n",
    "            citation_button = gr.Button(\"Show Citations\", interactive=False)\n",
    "            citation_text = gr.Textbox(label=\"Citations\", interactive=False)\n",
    "    \n",
    "    # Set the query input to trigger the search_query function\n",
    "    query_input.submit(search_query, inputs=query_input, outputs=[response_text, citation_button])\n",
    "    \n",
    "    # Set the button to show the citations when clicked\n",
    "    citation_button.click(show_citations, outputs=citation_text)\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(server_name=\"192.168.0.44\", server_port=8895)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4899771-12c4-449b-8481-246281f68f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is taming intuitive predictions ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
